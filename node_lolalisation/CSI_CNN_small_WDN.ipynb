{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# loading data\n",
    "# @param:\n",
    "#     path - the directory under which the numpy files are \n",
    "\n",
    "def load_data(path):\n",
    "    X = np.load(path+'full_data.npy')\n",
    "    Y = np.load(path+'pollution_data.npy')\n",
    "    return X, Y\n",
    "\n",
    "def iscontaminated(e, n_steps):\n",
    "    for j in range(len(e[0])-n_steps+1):\n",
    "        for i in range(len(e)):\n",
    "            if e[i][j] > 0:\n",
    "                return j\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# spliting data into training and test sets and re-organize data format according to given settings\n",
    "# @param:\n",
    "#     X - raw sensor data\n",
    "#     Y - one-hot labels\n",
    "#     n_sensors    - the number of sensors deployed in the WDS\n",
    "#     n_steps      - the number of report steps used for DNN model from when the contamination is detected\n",
    "#     random_state - the seed for random split\n",
    "#     test_size    - the ratio of test set\n",
    "\n",
    "def split_data(X, Y, n_sensors=10, n_steps=10, random_state=0, test_size = 0.2):\n",
    "    \n",
    "    SIZE = len(X)\n",
    "    STEPS = len(X[0][0])\n",
    "    \n",
    "    # extract data only from sensor nodes\n",
    "    sensor_index = [10 * i for i in range(n_sensors)]\n",
    "    sensor_data = np.tile(-1, (SIZE, n_sensors, STEPS))\n",
    "    for i in range(SIZE):\n",
    "        sensor_data[i] = X[i][[sensor_index]]\n",
    "        \n",
    "    # filter undetected events \n",
    "    cnt = 0\n",
    "    start_index = np.zeros(SIZE).astype(np.int)\n",
    "    for i in range(SIZE):\n",
    "        ind = iscontaminated(sensor_data[i], n_steps)\n",
    "        if ind >= 0:\n",
    "            cnt += 1\n",
    "        start_index[i] = ind\n",
    "\n",
    "    # extract data within limited time\n",
    "    limited_sensor_data = np.tile(-1, (cnt, n_sensors, n_steps))\n",
    "    limited_pollution_data = np.tile(-1,(cnt, 92))\n",
    "    \n",
    "    j = 0\n",
    "    for i in range(SIZE):\n",
    "        if start_index[i] >= 0:\n",
    "        \n",
    "            limited_sensor_data[j] = sensor_data[i,0:n_sensors,start_index[i]:start_index[i]+n_steps]\n",
    "            limited_pollution_data[j] = Y[i]\n",
    "            j += 1\n",
    "            \n",
    "    print(limited_sensor_data.shape)\n",
    "    \n",
    "    #split data into train set and test set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(limited_sensor_data, limited_pollution_data, random_state=random_state, test_size=test_size)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(x, y, batch_size):\n",
    "\n",
    "    c = 0\n",
    "    while True:\n",
    "        \n",
    "        if c > len(x) - batch_size:\n",
    "            c = 0\n",
    "\n",
    "        batch_x = x[c:c+batch_size]\n",
    "        batch_x = np.reshape(batch_x, (batch_x.shape[0], batch_x.shape[1], batch_x.shape[2], 1))\n",
    "        batch_y = y[c:c+batch_size]\n",
    "\n",
    "        yield batch_x, batch_y\n",
    "        c += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "x, y = load_data(\"./wds_data/data_10000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for N_S in [4,6]:\n",
    "    for N_T in [6,12]:\n",
    "        x_train_valid, x_test, y_train_valid, y_test = split_data(x, y, N_S, N_T)\n",
    "\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x_train_valid, y_train_valid, random_state=0, test_size=0.25)\n",
    "        print(x_train.shape, x_valid.shape, x_test.shape, y_train.shape, y_valid.shape, y_test.shape)\n",
    "\n",
    "        # ===========================\n",
    "\n",
    "        batch_size = 128\n",
    "        epochs = 120\n",
    "        steps_per_epoch = len(x_train) / batch_size \n",
    "\n",
    "        # input data dimensions\n",
    "        rows, cols = len(x_train[0]), len(x_train[0][0])\n",
    "\n",
    "        input_shape = (rows, cols, 1)\n",
    "        print('input_shape: ', input_shape)\n",
    "\n",
    "        num_classes = y_train.shape[1]\n",
    "        print('num_classes: ', num_classes)\n",
    "\n",
    "        train_generator = generate_data(x_train, y_train, batch_size)\n",
    "        validate_generator = generate_data(x_valid, y_valid, batch_size)\n",
    "        evaluate_generator = generate_data(x_test, y_test, batch_size)\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=(1, 2),\n",
    "                         activation='relu', padding='same', \n",
    "                         input_shape=input_shape))\n",
    "        model.add(Conv2D(32, (1, 2), activation='relu', padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (1, 2), activation='relu', padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, (1, 2), activation='relu', padding='same'))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # checkpoint\n",
    "        filepath='./trained_models/CSI_cnn_best_'+str(N_S)+'-'+str(N_T)+'.hdf5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "        callbacks_list = [checkpoint]\n",
    "        \n",
    "        model.fit_generator(train_generator, \n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=validate_generator, \n",
    "                    callbacks = callbacks_list,\n",
    "                    validation_steps=100)\n",
    "        model = load_model('./trained_models/CNN_trained_models/CSI_cnn_best_'+str(N_S)+'-'+str(N_T)+'.hdf5')\n",
    "        loss, accuracy = model.evaluate_generator( evaluate_generator, len(y_test)/batch_size )\n",
    "        print (\"Accuracy on test set is\", accuracy)\n",
    "        evaluator = generate_data(x_test, y_test, len(y_test))\n",
    "        predictions = model.predict_generator(evaluator, 1)\n",
    "        n = 5\n",
    "        topn = np.zeros(n)\n",
    "        cntn = np.zeros(n)\n",
    "        for i in range(len(predictions)):\n",
    "            for j in range(n):\n",
    "                topn[j] = np.argpartition(predictions[i], -(j+1))[-(j+1):][0]\n",
    "            for j in range(n):\n",
    "                if topn[j] == np.argmax(y_test[i]):\n",
    "                    cntn[j] += 1\n",
    "                    break\n",
    "        print(cntn / len(predictions))\n",
    "        results[(N_S, N_T, 1)] = cntn[0] / len(predictions)\n",
    "        print(sum(cntn) / len(predictions))\n",
    "        results[(N_S, N_T, 5)] = sum(cntn) / len(predictions)\n",
    "        print('results: ', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
